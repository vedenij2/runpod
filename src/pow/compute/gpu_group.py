import time
from typing import List, Dict, Optional
import torch
from pow.models.utils import PARAMS_V1, PARAMS_V2, Params
from common.logger import create_logger

logger = create_logger(__name__)

# Lazy import to avoid circular dependency
_gpu_arch_module = None


def _get_gpu_architecture(device_id: int = 0):
    """Lazy import of gpu_arch module to avoid circular imports."""
    global _gpu_arch_module
    if _gpu_arch_module is None:
        from pow.compute import gpu_arch
        _gpu_arch_module = gpu_arch
    return _gpu_arch_module.get_gpu_architecture(device_id)


def _is_blackwell_gpu(device_id: int = 0) -> bool:
    """Check if running on Blackwell GPU."""
    global _gpu_arch_module
    if _gpu_arch_module is None:
        from pow.compute import gpu_arch
        _gpu_arch_module = gpu_arch
    return _gpu_arch_module.is_blackwell_gpu(device_id)

# Retry settings for CUDA availability check at startup
CUDA_RETRY_ATTEMPTS = 5
CUDA_RETRY_DELAY_SECONDS = 1.0


class NotEnoughGPUResources(Exception):
    """Raised when GPU is not available or has insufficient resources"""
    pass


def get_min_group_vram(params: Params, device_id: int = 0) -> float:
    """
    Get minimum VRAM required per group, adjusted for GPU architecture.

    Blackwell GPUs can use memory more efficiently due to TMEM (Tensor Memory),
    so we can reduce the threshold slightly.

    Args:
        params: Model parameters
        device_id: GPU device ID for architecture detection

    Returns:
        Minimum VRAM in GB required for the GPU group
    """
    if params == PARAMS_V1:
        base_vram = 10.0
    elif params == PARAMS_V2:
        base_vram = 38.0
    else:
        base_vram = 38.0

    # Blackwell GPUs have more efficient memory management with TMEM
    try:
        if torch.cuda.is_available() and _is_blackwell_gpu(device_id):
            # Blackwell can operate with ~15% less VRAM requirement
            adjusted_vram = base_vram * 0.85
            logger.debug(
                f"Blackwell GPU detected: reduced VRAM threshold from "
                f"{base_vram:.1f}GB to {adjusted_vram:.1f}GB"
            )
            return adjusted_vram
    except Exception as e:
        logger.debug(f"Could not detect GPU architecture: {e}")

    return base_vram


class GpuGroup:
    def __init__(self, devices: List[int]):
        if not devices:
            raise ValueError("GPU group must have at least one device")

        self.devices = devices
        self.primary_device = devices[0]  # First device is primary
        self.group_size = len(devices)

    def __repr__(self):
        return f"GpuGroup(devices={self.devices}, primary={self.primary_device})"

    def get_device_strings(self) -> List[str]:
        return [f"cuda:{device_id}" for device_id in self.devices]

    def get_primary_device_string(self) -> str:
        return f"cuda:{self.primary_device}"

    def get_total_vram_gb(self) -> float:
        if not torch.cuda.is_available():
            return 0.0

        total_vram = 0.0
        for device_id in self.devices:
            if device_id < torch.cuda.device_count():
                props = torch.cuda.get_device_properties(device_id)
                total_vram += props.total_memory / (1024**3)  # Convert to GB
        return total_vram

    def get_free_vram_mb_per_device(self) -> Dict[int, int]:
        if not torch.cuda.is_available():
            return {device_id: 0 for device_id in self.devices}

        free_vram_map = {}
        for device_id in self.devices:
            if device_id < torch.cuda.device_count():
                free_mem_bytes, _ = torch.cuda.mem_get_info(device_id)
                free_vram_map[device_id] = int(free_mem_bytes / (1024**2))
            else:
                free_vram_map[device_id] = 0
        return free_vram_map

    def get_free_vram_gb(self) -> float:
        free_vram_per_device_mb = self.get_free_vram_mb_per_device()

        total_free_vram_mb = sum(free_vram_per_device_mb.values())

        return total_free_vram_mb / 1024


def _wait_for_cuda_available() -> bool:
    """
    Wait for CUDA to become available with retry logic.
    This is needed because CUDA may not be immediately available
    when the RunPod container starts.

    Returns:
        True if CUDA became available, False otherwise
    """
    for attempt in range(CUDA_RETRY_ATTEMPTS):
        if torch.cuda.is_available():
            if attempt > 0:
                logger.info(f"CUDA became available after {attempt + 1} attempts")
            return True

        if attempt < CUDA_RETRY_ATTEMPTS - 1:
            logger.warning(
                f"CUDA not available (attempt {attempt + 1}/{CUDA_RETRY_ATTEMPTS}), "
                f"waiting {CUDA_RETRY_DELAY_SECONDS}s..."
            )
            try:
                torch.cuda.empty_cache()
            except Exception:
                pass
            time.sleep(CUDA_RETRY_DELAY_SECONDS)

    return False


def create_gpu_groups(min_vram_gb: float = None, params: Params = None) -> List[GpuGroup]:

    if not _wait_for_cuda_available():
        error_msg = "CUDA is not available - no GPU support detected"
        logger.error(error_msg)
        raise NotEnoughGPUResources(error_msg)

    if min_vram_gb is None:
        min_vram_gb = get_min_group_vram(params)

    device_count = torch.cuda.device_count()
    if device_count == 0:
        error_msg = "No CUDA devices found - GPU count is 0"
        logger.error(error_msg)
        raise NotEnoughGPUResources(error_msg)

    # Get VRAM for each device, sorted by device_id for determinism
    device_vram = []
    for device_id in range(device_count):
        props = torch.cuda.get_device_properties(device_id)
        vram_gb = props.total_memory / (1024**3)
        device_vram.append((device_id, vram_gb))

    groups = []
    available_devices = list(device_vram)
    preferred_sizes = [1, 2, 4, 8]

    while available_devices:
        group_formed = False
        for group_size in preferred_sizes:
            if len(available_devices) >= group_size:
                potential_group_tuples = available_devices[:group_size]
                total_vram = sum(vram for _, vram in potential_group_tuples)

                if total_vram >= min_vram_gb:
                    device_ids = [device_id for device_id, _ in potential_group_tuples]
                    groups.append(GpuGroup(device_ids))
                    available_devices = available_devices[group_size:]
                    group_formed = True
                    break  # Found a valid group, move to next block of available devices

        if not group_formed:
            # Could not form a valid group starting with the current device.
            # Discard it and try to form a group from the remaining devices.
            discarded_device = available_devices.pop(0)
            logger.warning(f"GPU {discarded_device[0]} has insufficient VRAM ({discarded_device[1]:.1f}GB) to form a group, required: {min_vram_gb:.1f}GB")

    if not groups:
        device_info = ", ".join([f"GPU{device_id}: {vram:.1f}GB" for device_id, vram in device_vram])
        error_msg = f"Not enough GPU memory to form any groups - required: {min_vram_gb:.1f}GB per group, available: [{device_info}]"
        logger.error(error_msg)
        raise NotEnoughGPUResources(error_msg)

    return groups
