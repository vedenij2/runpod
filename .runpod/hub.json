{
  "title": "LLaMA GPU Benchmark",
  "description": "GPU performance benchmark using LLaMA model inference for measuring compute throughput",
  "type": "serverless",
  "category": "language",
  "config": {
    "runsOn": "GPU",
    "containerDiskInGb": 50,
    "gpuCount": 1,
    "allowedCudaVersions": ["12.8", "12.4", "12.1", "11.8"],
    "env": [
      {
        "name": "LOG_LEVEL",
        "type": "string",
        "default": "INFO",
        "description": "Logging level (DEBUG, INFO, WARNING, ERROR)"
      },
      {
        "name": "PYTORCH_CUDA_ALLOC_CONF",
        "type": "string",
        "default": "max_split_size_mb:512",
        "description": "PyTorch CUDA memory allocator configuration",
        "advanced": true
      },
      {
        "name": "BENCHMARK_FALLBACK_MODE",
        "type": "string",
        "default": "0",
        "description": "Set to 1 to disable architecture-specific optimizations (FP8, etc.)",
        "advanced": true
      }
    ]
  }
}
